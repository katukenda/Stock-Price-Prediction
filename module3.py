# -*- coding: utf-8 -*-
"""Module3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eSu9tbqMYYcZObJljGbAkQfKwmoCWgTr
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
plt.style.use("fivethirtyeight")
# %matplotlib inline

# For time stamps
from datetime import datetime

# The tech stocks we'll use for this analysis
tech_list = ['AAPL', 'AMZN', 'BAC', 'PFE']

# Set up End and Start times for data grab
end = datetime.now()
start = datetime(end.year - 1, end.month, end.day)

#import Data sets
AAPL = pd.read_csv("input/AAPL.csv")
AMZN = pd.read_csv("input/AMZN.csv")
BAC = pd.read_csv("input/BAC.csv")
PFE = pd.read_csv("input/PFE.csv")

company_list = [AAPL, AMZN, BAC, PFE]
company_name = ["APPLE", "AMZON", "BANKOFAMERICA", "PFIZER"]

# Summary Stats
AAPL.describe()

# General info
AAPL.info()

# Let's see a historical view of the closing price
plt.figure(figsize=(15, 6))
plt.subplots_adjust(top=1.25, bottom=1.2)

for i, company in enumerate(company_list, 1):
    plt.subplot(2, 2, i)
    company['Close'].plot()
    plt.ylabel('Close')
    plt.xlabel(None)
    plt.title(f"Closing Price of {tech_list[i - 1]}")
    
plt.tight_layout()

# Now let's plot the total volume of stock being traded each day
plt.figure(figsize=(15, 7))
plt.subplots_adjust(top=1.25, bottom=1.2)

for i, company in enumerate(company_list, 1):
    plt.subplot(2, 2, i)
    company['Volume'].plot()
    plt.ylabel('Volume')
    plt.xlabel(None)
    plt.title(f"Sales Volume for {tech_list[i - 1]}")
    
plt.tight_layout()

#moving averages
ma_day = [10, 20, 50]

for ma in ma_day:
    for company in company_list:
        column_name = f"MA for {ma} days"
        company[column_name] = company['Close'].rolling(ma).mean()

fig, axes = plt.subplots(nrows=2, ncols=2)
fig.set_figheight(8)
fig.set_figwidth(15)

AAPL[['Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,0])
axes[0,0].set_title('APPLE')

AMZN[['Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,1])
axes[0,1].set_title('AMZON')

BAC[['Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,0])
axes[1,0].set_title('BANKOFAMERICA')

PFE[['Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,1])
axes[1,1].set_title('PFIZER')

fig.tight_layout()

# We'll use pct_change to find the percent change for each day
for company in company_list:
    company['Daily Return'] = company['Close'].pct_change()

# Then we'll plot the daily return percentage
fig, axes = plt.subplots(nrows=2, ncols=2)
fig.set_figheight(8)
fig.set_figwidth(15)

AAPL['Daily Return'].plot(ax=axes[0,0], legend=True, linestyle='--', marker='o')
axes[0,0].set_title('APPLE')

AMZN['Daily Return'].plot(ax=axes[0,1], legend=True, linestyle='--', marker='o')
axes[0,1].set_title('AMAZON')

BAC['Daily Return'].plot(ax=axes[1,0], legend=True, linestyle='--', marker='o')
axes[1,0].set_title('BANKOFAMERICA')

PFE['Daily Return'].plot(ax=axes[1,1], legend=True, linestyle='--', marker='o')
axes[1,1].set_title('PFIZER')

fig.tight_layout()

# Note the use of dropna() here, otherwise the NaN values can't be read by seaborn
plt.figure(figsize=(12, 7))

for i, company in enumerate(company_list, 1):
    plt.subplot(2, 2, i)
    sns.distplot(company['Daily Return'].dropna(), bins=100, color='purple')
    plt.ylabel('Daily Return')
    plt.title(f'{company_name[i - 1]}')
# Could have also done:
#AAPL['Daily Return'].hist()
plt.tight_layout()

AAPL = pd.read_csv("input/AAPL.csv")
AMZN = pd.read_csv("input/AMZN.csv")
BAC = pd.read_csv("input/BAC.csv")
PFE = pd.read_csv("input/PFE.csv")
data = [AAPL["Close"], AMZN["Close"], BAC["Close"], PFE["Close"]]
headers = ['AAPL', 'AMZN', 'BAC', 'PFE']
closing_df = pd.concat(data, axis=1, keys=headers)
closing_df.head()

# Make a new tech returns DataFrame
tech_rets = closing_df.pct_change()
tech_rets.head()

# Comparing Apple to itself should show a perfectly linear relationship
sns.jointplot('AAPL', 'AAPL', tech_rets, kind='scatter', color='seagreen')

# Let's start by defining a new DataFrame as a clenaed version of the oriignal tech_rets DataFrame
rets = tech_rets.dropna()

area = np.pi * 20

plt.figure(figsize=(10, 7))
plt.scatter(rets.mean(), rets.std(), s=area)
plt.xlabel('Expected return')
plt.ylabel('Risk')

for label, x, y in zip(rets.columns, rets.mean(), rets.std()):
    plt.annotate(label, xy=(x, y), xytext=(50, 50), textcoords='offset points', ha='right', va='bottom', 
                 arrowprops=dict(arrowstyle='-', color='blue', connectionstyle='arc3,rad=-0.3'))

"""Prediction Stock price without twitter sentiment"""

# Predicting Stocks without sentiments

import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import io
import warnings
from sklearn import metrics
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

import io
num_epochs = 100
batch_size = 32
split = 0.4

df = pd.read_csv(io.StringIO(uploaded['AAPL.csv'].decode('utf-8')))
df

# considering the open price
df = df.drop(df.columns[2:],axis=1)
df.head()

# lets check the dataset through a graph
#df['Open'].plot(figsize=(12,6))


# Feature Scaling
training = split
total = df.shape[0]
train_index = int(total*training)

training_set = df[:train_index]

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(pd.DataFrame(training_set['Open']))

# getting Specified time
X_train = []
y_train = []
for i in range(60, train_index):
    X_train.append(training_set_scaled[i-60:i, 0])
    y_train.append(training_set_scaled[i, 0])
X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# LSTM Model Implementation
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

model = Sequential()
model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.2))
model.add(LSTM(units = 50))
model.add(Dropout(0.2))
model.add(Dense(units = 1))
model.compile(optimizer = 'adam', loss = 'mean_squared_error')
model.fit(X_train, y_train, epochs = num_epochs, batch_size = batch_size)

testing_set = df[train_index:]
testing_set = pd.DataFrame(testing_set)
testing_set.head()

# real stock
real_stock_price = pd.DataFrame(testing_set['Open']).reset_index(drop=True)
test_set=testing_set['Open']
test_set=pd.DataFrame(test_set)

inputs = df['Open'][len(df) - len(testing_set) - 60:].values
# print(inputs)
inputs = inputs.reshape(-1,1)
# print(inputs)
inputs = sc.transform(inputs)
# print(inputs)
X_test = []
for i in range(60, 60+(total-train_index)):
    X_test.append(inputs[i-60:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

#predictions
predicted_stock_price = model.predict(X_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)

predicted_stock_price=pd.DataFrame(predicted_stock_price)
predicted_stock_price_old = predicted_stock_price

# plot real stock vs predicted stock
plt.rcParams['figure.figsize'] = [12, 10]
plt.plot(real_stock_price, color = 'red', label = 'Real Stock Price')
plt.plot(predicted_stock_price_old, color = 'green', label = 'Predicted Stock Price')
plt.title('Apple Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Apple Stock Price')
plt.legend()
plt.show()

"""Stock prediction with sentiment"""

# Commented out IPython magic to ensure Python compatibility.
# predicting stock price with sentiments
!pip install hvplot
import numpy as np
import pandas as pd
import hvplot.pandas
# %matplotlib inline
from sklearn import metrics

from google.colab import files
uploaded = files.upload()
import io

# setting seeds f
from numpy.random import seed
seed(1)
from tensorflow import random
random.set_seed(2)

#df = pd.read_csv('AAPL_with_Sentiments.csv', index_col="Date", infer_datetime_format=True, parse_dates=True)
df = pd.read_csv(io.StringIO(uploaded['AAPL_with_Sentiments.csv'].decode('utf-8')),index_col="Date", infer_datetime_format=True, parse_dates=True)
# Drop null values
df.dropna(inplace=True)
df.head()

# preparing data, training and testing data

def prepare_data(df, window, feature_col_number1, feature_col_number2, feature_col_number3, target_col_number):
    # Create empty lists "X_close", "X_polarity", "X_volume" and y
    X_close = []
    X_polarity = []
    X_volume = []
    y = []
    for i in range(len(df) - window):
        
        # Get close, ts_polarity, tw_vol, and target in the loop
        close = df.iloc[i:(i + window), feature_col_number1]
        ts_polarity = df.iloc[i:(i + window), feature_col_number2]
        tw_vol = df.iloc[i:(i + window), feature_col_number3]
        target = df.iloc[(i + window), target_col_number]
        
        # Append values in the lists
        X_close.append(close)
        X_polarity.append(ts_polarity)
        X_volume.append(tw_vol)
        y.append(target)
        
    return np.hstack((X_close,X_polarity,X_volume)), np.array(y).reshape(-1, 1)


window_size = 3

# Column index 0 is the `Adj Close` column
# Column index 1 is the `ts_polarity` column
# Column index 2 is the `twitter_volume` column
feature_col_number1 = 0
feature_col_number2 = 1
feature_col_number3 = 2
target_col_number = 0
X, y = prepare_data(df, window_size, feature_col_number1, feature_col_number2, feature_col_number3, target_col_number)

# Use 70% of the data for training and the remaineder for testing
X_split = int(0.7 * len(X))
y_split = int(0.7 * len(y))

X_train = X[: X_split]
X_test = X[X_split:]
y_train = y[: y_split]
y_test = y[y_split:]

#Data Scaling

from sklearn.preprocessing import MinMaxScaler
# Use the MinMaxScaler to scale data between 0 and 1.
x_train_scaler = MinMaxScaler()
x_test_scaler = MinMaxScaler()
y_train_scaler = MinMaxScaler()
y_test_scaler = MinMaxScaler()

# Fit the scaler for the Training Data
x_train_scaler.fit(X_train)
y_train_scaler.fit(y_train)

# Scale the training data
X_train = x_train_scaler.transform(X_train)
y_train = y_train_scaler.transform(y_train)

# Fit the scaler for the Testing Data
x_test_scaler.fit(X_test)
y_test_scaler.fit(y_test)

# Scale the y_test data
X_test = x_test_scaler.transform(X_test)
y_test = y_test_scaler.transform(y_test)

# Reshape again
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Build the LSTM model. 

# Define the LSTM RNN model.
model = Sequential()

number_units = 50
dropout_fraction = 0.2

# Layer 1
model.add(LSTM(
    units=number_units,
    return_sequences=True,
    input_shape=(X_train.shape[1], 1))
    )
    
model.add(Dropout(dropout_fraction))

# Layer 2
# The return_sequences parameter needs to set to True every time we add a new LSTM layer, excluding the final layer.
model.add(LSTM(units=number_units, return_sequences=True))
model.add(Dropout(dropout_fraction))

# Layer 3
model.add(LSTM(units=number_units))
model.add(Dropout(dropout_fraction))

# Output layer
model.add(Dense(1))

# Compile the model
model.compile(optimizer="adam", loss="mean_squared_error")

# Summarize the model
model.summary()

# Train model
model.fit(X_train, y_train, epochs=10, shuffle=False, batch_size=5, verbose=1)


# Evaluate the model for loss
model.evaluate(X_test, y_test)

# Make some predictions
predicted = model.predict(X_test)


predicted_prices = y_test_scaler.inverse_transform(predicted)
real_prices = y_test_scaler.inverse_transform(y_test.reshape(-1, 1))

# Create a DataFrame of Real and Predicted values
stocks = pd.DataFrame({"Real": real_prices.ravel(),"Predicted": predicted_prices.ravel()}, index = df.index[-len(real_prices): ]) 
stocks.head()

stocks.plot()

#without sentiment
print('Without Sentiment.........')
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(real_stock_price, predicted_stock_price_old)))
print('Mean Absolut Error  :', metrics.mean_absolute_error(real_stock_price, predicted_stock_price_old))
print('Mean Squared Error:', metrics.mean_squared_error(real_stock_price, predicted_stock_price_old))
#with sentiment
print('With Sentiment.........')
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predicted)))
print('Mean Absolut Error :', metrics.mean_absolute_error(y_test, predicted))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, predicted))